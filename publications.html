<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Selected Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">home</a></div>
<div class="menu-item"><a href="news.html">news</a></div>
<div class="menu-item"><a href="publications.html" class="current">publications</a></div>
<div class="menu-item"><a href="collaborations.html">collaborations</a></div>
<div class="menu-item"><a href="patents.html">patents</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Selected Publications</h1>
</div>
<p>Below, I use “^” to indicate the student collaborator that I mentored during whose internship or during an university collaboration.<br />
[<a href="http://scholar.google.com/citations?user=0cygTk0AAAAJ&amp;hl=en">Google Scholar</a>] [<a href="https://dblp.org/pers/r/Ren:Zhou.html">DBLP</a>] <br /></p>
<h2>1. <i>Hand, Gesture, and Human Pose</i></h2>
<table class="imgtable"><tr><td>
<img src="paper2.png" alt="paper6" width="410cm" height="200cm" />&nbsp;</td>
<td align="left"></td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p><b>Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking</b><br />
Yiding Yang^, <i>Zhou Ren</i>, Haoxiang Li, Chunluan Zhou, Xinchao Wang, and Gang Hua<br />
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.<br />
[<a href="GNN_Dynamics_PoseTracking.pdf">PDF</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Temporal Keypoint Matching and Refinement Network for Pose Estimation and Tracking</b><br />
Chunluan Zhou, <i>Zhou Ren</i>, and Gang Hua<br />
In IEEE European Conference on Computer Vision (ECCV), 2020.<br />
[<a href="ECCV2020_poseTracking.pdf">PDF</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>3D Hand Shape and Pose Estimation from a Single RGB Image</b><br />
Liuhao Ge^, <i>Zhou Ren</i>, Yuncheng Li, Zehao Xue, Yingying Wang, Jianfei Cai, Junsong Yuan<br />
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019 (<b>Oral</b>).<br />
[<a href="3DhandShapePoseEst_CVPR_2019_paper.pdf">PDF</a>][<a href="3DhandShapePoseEst_CVPR_2019_supp.pdf">supplementary</a>][<a href="https://www.youtube.com/watch?v=NActf7FcrmI&amp;feature=youtu.be">video</a>][<a href="https://github.com/3d-hand-shape/hand-graph-cnn">code</a>][<a href="https://github.com/3d-hand-shape/hand-graph-cnn/tree/master/data">dataset</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>End-to-End 3D Hand Pose Estimation from Stereo Cameras</b><br /> 
Yuncheng Li, Zehao Xue, Yingying Wang, Liuhao Ge, <i>Zhou Ren</i>, Jonathan Rodriguez<br /> 
In British Machine Vision Conference (BMVC), 2019 (<b>Oral</b>).<br />
[<a href="https://bmvc2019.org/wp-content/uploads/papers/0219-paper.pdf">PDF</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Point-to-Point Regression PointNet for 3D Hand Pose Estimation</b><br />
Liuhao Ge^, <i>Zhou Ren</i>, and Junsong Yuan<br />
In European Conference on Computer Vision (ECCV), 2018.<br />
[<a href="Point-to-Point_Regression_PointNet_ECCV_2018_paper.pdf">PDF</a>] </p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Robust Part-based Hand Gesture Recognition Using Kinect Sensor</b><br />
<i>Zhou Ren</i>, Junsong Yuan, Jingjing Meng, and Zhengyou Zhang<br />
In IEEE Trans. on Multimedia (TMM), 15(5), 1110-1120, 2013.<br />
* <b><i>Winner of <a href="http://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia/multimedia-prize-paper-award">2016 IEEE Trans. on Multimedia Prize Paper Award (Best Paper Award)</a>*</i></b><br />
[<a href="TMM_RenYuanMengZhang_submission.pdf">PDF</a>][<a href="Ren_TMM13.bib">Bibtex</a>][<a href="https://drive.google.com/file/d/1f8tUHid1KmnwbgskGMXmobOxMfbxIgHM/view?usp=sharing">NTU-Microsoft-Kinect HandGesture Dataset</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Robust Hand Gesture Recognition based on Finger-Earth Mover’s Distance with a Commodity Depth Camera</b><br />
<i>Zhou Ren</i>, Junsong Yuan, and Zhengyou Zhang<br />
In ACM Multimedia (ACM MM), Scottsdale, Arizona, USA, Nov. 28-Dec. 1, 2011.<br />
<b><i><a href="https://aminer.org/conferencebestpapers">*The most cited paper in ACM MM 2011*</a></i></b><br />
[<a href="Ren_Yuan_Zhang_MM11short.pdf">PDF</a>][<a href="Ren_MM11a.bib">Bibtex</a>][<a href="https://drive.google.com/file/d/1f8tUHid1KmnwbgskGMXmobOxMfbxIgHM/view?usp=sharing">NTU-Microsoft-Kinect HandGesture Dataset</a>][<a href="https://www.youtube.com/watch?v=m0IwsxUpJFU">Demo</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Robust Hand Gesture Recognition with Kinect Sensor</b><br />
<i>Zhou Ren</i>, Jingjing Meng, Junsong Yuan, and Zhengyou Zhang<br />
In ACM Multimedia (ACM MM), Scottsdale, Arizona, USA, Nov. 28-Dec. 1, 2011.<br />
<b><i><a href="https://aminer.org/conferencebestpapers">*The 2nd most cited paper in ACM MM 2011*</a></i></b><br />
[<a href="Ren_Meng_Yuan_Zhang_MM11.pdf">PDF</a>][<a href="Ren_MM11b.bib">Bibtex</a>][<a href="https://www.youtube.com/watch?v=m0IwsxUpJFU">Demo</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Depth Camera based Hand Gesture Recognition and its Applications in Human-Computer-Interaction</b><br />
<i>Zhou Ren</i>, Jingjing Meng, and Junsong Yuan<br />
In IEEE International Conference on Information, Communication, and Signal Processing (ICICS), Singapore, Dec. 2011 (<b>Oral</b>).<br />
[<a href="Ren_Meng_Yuan_ICICS2011.pdf">PDF</a>][<a href="Ren_ICICS11.bib">Bibtex</a>] [<a href="https://www.youtube.com/watch?v=WJKsxGmPNwE">Demo1</a>] [<a href="https://www.youtube.com/watch?v=VnFtT1vtMsU">Demo2</a>]</p>
</div></div>
<h2><b>2</b>. <i><b>Object Detection, Action Detection, and Person ReID</b></i></h2>
<table class="imgtable"><tr><td>
<img src="ActionDetection.png" alt="ActionDetection" width="310cm" height="180cm" />&nbsp;</td>
<td align="left"></td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p><b>SaccadeNet: a Fast and Accurate Object Detector</b><br />
Shiyi Lan^, <i>Zhou Ren</i>, Yi Wu, Larry Davis, and Gang Hua<br />
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.<br />
[<a href="SaccadeNet_CVPR2020.pdf">PDF</a>][<a href="https://github.com/voidrank/SaccadeNet">code</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Calibrated domain-invariant learning for highly generalizable large scale re-identification</b><br />
Ye Yuan^, Wuyang Chen^, Tianlong Chen^, Yang Yang, <i>Zhou Ren</i>, Zhangyang Wang, Gang Hua<br />
In IEEE Winter Conference on Applications of Computer Vision (WACV), 2020.<br />
[<a href="https://openaccess.thecvf.com/content_WACV_2020/html/Yuan_Calibrated_Domain-Invariant_Learning_for_Highly_Generalizable_Large_Scale_Re-Identification_WACV_2020_paper.html">PDF</a>][<a href="https://github.com/TAMU-VITA/ADIN">code</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>ABD-Net: Attentive but Diverse Person Re-Identification</b><br />
Tianlong Chen^, Shaojin Ding, Jingyi Xie, Ye Yuan^, Wuyang Chen^, Yang Yang, <i>Zhou Ren</i>, and Zhangyang Wang<br />
In International Conference on Computer Vision (ICCV), 2019.<br />
[<a href="https://arxiv.org/abs/1908.01114">PDF</a>][<a href="https://github.com/TAMU-VITA/ABD-Net">code</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Temporal Structure Mining for Weakly Supervised Action Detection</b><br />
Tan Yu^, <i>Zhou Ren</i>, Yuncheng Li, Enxu Yan, Ning Xu, and Junsong Yuan<br />
In International Conference on Computer Vision (ICCV), 2019.<br />
[<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Temporal_Structure_Mining_for_Weakly_Supervised_Action_Detection_ICCV_2019_paper.pdf">PDF</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Deep Regionlets: Blended Representation and Deep Learning for Generic Object Detection</b><br />
Hongyu Xu^, Xutao Lv, Xiaoyu Wang, <i>Zhou Ren</i>, and Rama Chellappa<br />
In IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2019.<br />
[<a href="https://arxiv.org/abs/1811.11318">PDF</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Deep Regionlets for Object Detection</b><br />
Hongyu Xu^, Xutao Lv, Xiaoyu Wang, <i>Zhou Ren</i>, and Rama Chellappa<br />
In European Conference on Computer Vision (ECCV), 2018.<br />
[<a href="https://arxiv.org/abs/1712.02408">PDF</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Scene-Domain Active Part Models for Object Representation</b><br />
<i>Zhou Ren</i>, Chaohui Wang, and Alan Yuille<br />
In International Conference on Computer Vision (ICCV), 2015.<br />
[<a href="Ren_ICCV15.pdf">PDF</a>][<a href="Ren_etal_ICCV15.bib">Bibtex</a>]</p>
</div></div>
<h2><b>3</b>. <i><b>Multi-Modal Joint Understanding, Vision and Language</i></b></h2>
<table class="imgtable"><tr><td>
<img src="paperarXiv15.png" alt="paper_arXiv15" width="460cm" height="120cm" />&nbsp;</td>
<td align="left"></td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p><b>SibNet: Sibling Convolutional Encoder for Video Captioning</b><br /> 
Sheng Liu^, <i>Zhou Ren</i>, and Junsong Yuan; 
In IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2019.<br />
[<a href="https://cse.buffalo.edu/~jsyuan/papers/2020/SibNet_PAMI.pdf">PDF</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Streamlined Dense Video Captioning</b><br />
Jonghwan Mun^, Linjie Yang, <i>Zhou Ren</i>, Ning Xu, and Bohyung Han<br />
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019 (<b>Oral</b>).<br />
[<a href="StreamlinedDenseVideoCaptioning_CVPR_2019_paper.pdf">PDF</a>][<a href="StreamlinedDenseVideoCaptioning_CVPR_2019_supp.pdf">supplementary</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>SibNet: Sibling Convolutional Encoder for Video Captioning</b><br />
Sheng Liu^, <i>Zhou Ren</i>, and Junsong Yuan<br />
In ACM Multimedia, 2018 (<b>Oral</b>)<br />
[<a href="SibNet__Sibling_Convolutional_Encoder_for_Video_Captioning.pdf">PDF</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Multiple Instance Visual-Semantic Embedding</b><br />
<i>Zhou Ren</i>, Hailin Jin, Zhe Lin, Chen Fang, and Alan Yuille<br />
In British Machine Vision Conference (BMVC), 2017 (<b>Oral</b>)<br />
[<a href="Zhou_bmvc17_paper.pdf">PDF</a>][<a href="Zhou_bmvc17_Supplementary.pdf">Supplementary</a>][<a href="Ren_etal_BMVC_17.bib">Bibtex</a>][<a href="https://www.youtube.com/watch?v=ifltSty0kWw">Video</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Deep Reinforcement Learning-based Image Captioning with Embedding Reward</b><br />
<i>Zhou Ren</i>, Xiaoyu Wang, Ning Zhang, Xutao Lv, and Li-Jia Li<br />
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017 (<b>Oral</b>)<br />
<b><i> *Best Student Paper Award Nomination*</i></b><br />
[<a href="https://arxiv.org/pdf/1704.03899.pdf">PDF</a>][<a href="Ren_etal_CVPR_17.bib">Bibtex</a>][<a href="Zhou_CVPR17_talk.pdf">Talk slides</a>][<a href="Zhou_CVPR17_POSTER.pdf">Poster</a>][<a href="https://www.youtube.com/watch?v=iTpImCJRwks">Video</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Joint Image-Text Representation by Gaussian Visual Semantic Embedding</b><br />
<i>Zhou Ren</i>, Hailin Jin, Zhe Lin, Chen Fang, and Alan Yuille<br />
In ACM Multimedia Conference, 2016<br />
[<a href="GVSE_16.pdf">PDF</a>][<a href="Ren_etal_GVSE_16.bib">Bibtex</a>]</p>
</div></div>
<h2><b>4</b>. <i><b>Adversarial Machine Learning</i></b></h2>
<table class="imgtable"><tr><td>
<img src="paperICLR18.png" alt="paperICLR18" width="410cm" height="120cm" />&nbsp;</td>
<td align="left"></td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p><b>Improving Transferability of Adversarial Examples with Input Diversity</b><br />
Cihang Xie^, Yuyin Zhou, Song Bai, Zhishuai Zhang, Jianyu Wang, <i>Zhou Ren</i>, and Alan Yuille<br />
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.<br />
[<a href="https://arxiv.org/pdf/1803.06978.pdf">PDF</a>][<a href="https://github.com/cihangxie/DI-2-FGSM">code</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Mitigating Adversarial Effects Through Randomization</b><br />
Cihang Xie^, Jianyu Wang, Zhishuai Zhang, <i>Zhou Ren</i>, and Alan Yuille<br />
In International Conference on Learning Representations (ICLR), 2018<br />
* <b><i>Runner-up Winner in NIPS 2017 Adversarial Attack and Defense Competition (among 107 teams)*</i></b><br />
[<a href="https://arxiv.org/abs/1711.01991">PDF</a>][<a href="https://github.com/cihangxie/NIPS2017_adv_challenge_defense">code</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Adversarial Attacks and Defences Competition</b><br />
Alexey Kurakin, <i>et. al.</i><br />
In a book chapter from the NIPS 2017 Competition Book, Springer 2018<br />
[<a href="https://arxiv.org/abs/1804.00097">PDF</a>]</p>
</div></div>
<h2><b>5</b>. <i><b>Shape Representation, and Shape Coding</b></i></h2>
<table class="imgtable"><tr><td>
<img src="paper1.png" alt="paper1" width="310cm" height="180cm" />&nbsp;</td>
<td align="left"></td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p><b>Minimum Near-Convex Shape Decomposition</b><br />
<i>Zhou Ren</i>, Junsong Yuan, Wenyu Liu<br />
In IEEE Trans. on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 35(10), 2546-2552, 2013.<br />
[<a href="TPAMI_RenYuanLiu_submission.pdf">PDF</a>][<a href="Ren_TPAMI13.bib">Bibtex</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Minimum Near-Convex Decomposition for Robust Shape Representation</b><br />
<i>Zhou Ren</i>, Junsong Yuan, Chunyuan Li and Wenyu Liu<br />
In International Conference on Computer Vision (ICCV), pp.303-310, Barcelona, Spain, Nov. 2011.<br />
[<a href="Ren_Yuan_Li_Liu_ICCV11.pdf">PDF</a>][<a href="Ren_ICCV11.bib">Bibtex</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Arbitrary Directional Edge Encoding Schemes for the Operational Rate Distortion Optimal Shape Coding Framework</b><br />
Zhongyuan Lai, Junhuan Zhu, <i>Zhou Ren</i>, Wenyu Liu, and Baolan yan<br />
In IEEE Data Compression Conference (DCC), pp.20-29, Salt Lake City, USA, Nov. 2010 (<b>Oral</b>).<br />
[<a href="DCC10.pdf">PDF</a>][<a href="Lai_DCC10.bib">Bibtex</a>]</p>
</div></div>
<h2><b>6</b>. <i><b>Medical Image Processing</b></i></h2>
<table class="imgtable"><tr><td>
<img src="paper_medical.png" alt="paper_m" width="410cm" height="100cm" />&nbsp;</td>
<td align="left"></td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p><b>Automated Pericardial Fat Quantification from Coronary Magnetic Resonance Angiography: A Feasibility Study</b><br />
Xiaowei Ding, Jianing Pang, <i>Zhou Ren</i>, Mariana Diaz-Zamudio, Chenfangfu Jiang, Zhaoyang Fan, Daniel Berman, Debiao Li, Demetri Terzopoulos, Piotr Slomka, and Damini Dey<br />
In Journal of Medical Imaging, 2016.<br />
[<a href="Ding_JMI_2016.pdf">PDF</a>][<a href="Ding_etal_JMI16.bib">Bibtex</a>]</p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p><b>Automated Pericardial Fat Quantification from Coronary Magnetic Resonance Angiography</b><br />
Xiaowei Ding, Jianing Pang, <i>Zhou Ren</i>, Mariana Diaz-Zamudio, Daniel Berman, Debiao Li, Demetri Terzopoulos, Piotr Slomka, and Damini Dey<br />
In Medical Image Understanding and Analysis (MIUA), 2015 (<b>Oral</b>).<br />
[<a href="Ding_MIUA_2015.pdf">PDF</a>][<a href="Ding_etal_MIUA15.bib">Bibtex</a>]</p>
</div></div>
</td>
</tr>
</table>
</body>
</html>
